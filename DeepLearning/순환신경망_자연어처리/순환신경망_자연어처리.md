#   1장 신경망 복습
## 1.1 수학과 파이썬 복습
- ### 벡터와 행렬
- ### 행렬의 원소별 연산
- ### 브로드캐스트
  
## 1.2 신경망의 추론
## 1.3 신경망의 학습
## 1.4 신경망으로 문제를 풀다
## 1.5 계산 고속화
## 1.6 정리

#   2장 자연어와 단어의 분산 표현
## 2.1 자연어 처리란
## 2.2 시소러스
## 2.3 통계 기반 기법
## 2.4 통계 기반 기법 개선하기
## 2.5 정리

#   3장 word2vec
## 3.1 추론 기반 기법과 신경망
## 3.2 단순한 word2vec
## 3.3 학습 데이터 준비
## 3.4 CBOW 모델 구현
## 3.5 word2vec 보충
## 3.6 정리

# 4장 word2vec 속도 개선
## 4.1 word2vec 개선 ①
## 4.2 word2vec 개선 ②
## 4.3 개선판 word2vec 학습
## 4.4 word2vec 남은 주제
## 4.5 정리

# 5장 순환 신경망(RNN)
## 5.1 확률과 언어 모델
- 윈도우 크기가 1인 CBOW 모델의 사후 확률 모델링

- 좌우 비대칭으로도 윈도우 크기 설정 가능(ex:왼쪽 2, 오른쪽 0)
- CBOW 모델의 학습 -> 손실 함수를 최소화하는 가중치 매개변수 찾기 -> 학습의 부산물인 단어의 분산 표현 얻기
- 언어 모델(Language Model): 단어 나열에 확률을 부여, 특정한 단어의 시퀀스에 대해 그 시퀀스가 일어날 가능성이 어느 정도인지, 얼마나 자연스러운 시퀀스인지 확률로 평가
ex) 기계 번역, 음성 인식(문장으로써 자연스러운지를 기준으로 순서 매김), 새로운 문장을 생성(확률분포에 따른 적한한 단어 샘플링 가능)
- 언어 모델의 동시 확률 = 사후 확률의 총곱(타깃 단어보다 왼쪽에 있는 모든 단어를 맥락으로)
- 조건부 언어 모델
- 위의 사후 확률을 어떻게 구할 수 있을까?
- CBOW에서는 맥락의 크기를 특정 값으로 한정하여 근사적으로 나타낼 수 있다
여기서는 왼쪽 2개 단어로 한정
2층 마르코프 연쇄(Markov Chain: 미래의 상태가 현재 상태에만 의존해 결정되는 것)
-> 결국 특정 길이로 고정, 긴 맥락이 필요한 문제에 제대로 된 답을 도출 못함, 맥락 크기를 키울 수는 있으나 맥락 안의 단어 순서가 무시된다(bag-of-words)
## 5.2 RNN이란
## 5.3 RNN 구현
## 5.4 시계열 데이터 처리 계층 구현
## 5.5 RNNLM 학습과 평가
## 5.6 정리

# 6장 게이트가 추가된 RNN
## 6.1 RNN의 문제점
## 6.2 기울기 소실과 LSTM
## 6.3 LSTM 구현
## 6.4 LSTM을 사용한 언어 모델
## 6.5 RNNLM 추가 개선
## 6.6 정리

# 7장 RNN을 사용한 문장 생성
## 7.1 언어 모델을 사용한 문장 생성
## 7.2 seq2seq
## 7.3 seq2seq 구현
## 7.4 seq2seq 개선
## 7.5 seq2seq를 이용하는 애플리케이션
## 7.6 정리

# 8장 어텐션
## 8.1 어텐션의 구조
## 8.2 어텐션을 갖춘 seq2seq 구현
## 8.3 어텐션 평가
## 8.4 어텐션에 관한 남은 이야기
## 8.5 어텐션 응용
## 8.6 정리

APPENDIX A 시그모이드 함수와 tanh 함수의 미분
## A.1 시그모이드 함수
## A.2 tanh 함수
## A.3 정리

APPENDIX B WordNet 맛보기
## B.1 NLTK 설치
## B.2 WordNet에서 동의어 얻기
## B.3 WordNet과 단어 네트워크
## B.4 WordNet을 사용한 의미 유사도

APPENDIX C GRU
## C.1 GRU의 인터페이스
## C.2 GRU의 계산 그래프
