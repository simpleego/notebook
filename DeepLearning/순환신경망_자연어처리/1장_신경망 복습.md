# Chapter1: 신경망 복습
> 신경망은 일종의 함수이다.
> 입력층 + 은닉층 + 출력층

> 은닉층의 뉴련 = numpy.matmul(입력값,가중치)+편향
> 요렇게 구할 수 있다

- 완전연결계층 - 인접하는 층의 모든 뉴련과 연결된 신경망, 완전연결계층에 의한 변환은 선형 변환
- 활성화 함수(activation fuction) - 비선형 효과 부여 -> 신경망의 표현력 up

- 활성화 함수의 예시로는 시그모이드(sigmoid) 함수
![image](https://github.com/user-attachments/assets/56fb45da-7afe-41ff-af8d-444acdb40a4d)

(사진 출처: https://medium.com/@kmkgabia/ml-sigmoid-대신-relu-상황에-맞는-활성화-함수-사용하기-c65f620ad6fd)

minibatch 학습 - 전체 데이터를 작은 그룹으로 나눠 그룹 단위로 반복 학습하는 방식

신경망에서 최종 출력된 벡터의 각 차원의 값은 대응하는 클래스가 될 확률 이전의 점수값이 된다.
즉 차원들 중 가장 높은 값에 해당되는 클래스가 예측 결과가 된다.

점수 -> softmax function -> 확률
affine 변환

순전파(forward propagation) - 입력층에서 출력층으로
역전파(backward propagation) - 출력층에서 입력층으로

학습 먼저 -> 학습된 매개변수 바탕으로 추론
추론 - 다중 클래스 분류 등의 문제에 답을 구하는 작업
학습 - 최적의 매개변수 값을 찾는 작업

학습에서는,, 사람도 공부 잘하고 있나 확인하는 것처럼 여기서도 잘 배우고 있나 확인해야 한다고 한다,, 그거를 척도라고 하는데
"손실(loss)"을 척도로 사용한다

> 손실이란 학습 시 주어진 정답 데이터와 신경망이 여태까지 공부해서 예측한 결과를 비교해서 얼마나 잘못된 예측을 했는가를 산출한 스칼라 값
그렇다고 하네요,,

그러면 당연히 손실을 구하기 위해 손실 함수가 있을 것이고,,
다중 클래스 분류 신경망에서는 손실 함수로 교차 엔트로피 오차(cross entropy error)를 사용한다고 한다
그렇구만 그니까 이 책에서 예시로 드는 것도 이 손실 함수를 써야 하네

softmax 함수식은 뭐 여기저기 많이 나와있으니까 그 의미만 이야기하자면!
출력이 총 n개이고 k번째 출력값 즉 k번째 클래스에 해당하는 확률이 softmax 함수 출력값이다. 출력값은 0.0 ~ 1.0 사이의 실수

교차 엔트로피 오차값은 정답 레이블과 softmax 함수 출력값인 확률로 구할 수 있다.

근데 여기서 정답 레이블을 one-hot vector로 표기한다는데

> one-hot vector는 정답 클래스 원소만 1이고 나머지는 모두 0인 벡터
라고 한다.그렇군

그래서 학습의 최종 목표는 뭐냐면 손실을 당연히 최소화하는 매개변수를 찾아야 한다.
여기에 또 필요한게 미분과 기울기 안돼
여기서 가장 중요한 거는 다음 두 가지

- 행렬과 행렬의 기울기의 형상은 같다 -> 매개변수 갱신과 연쇄 법칙을 쉽게 구현할 수 있다고 하는데 아직은 잘 모르겠음,,

- 딥러닝에서의 기울기는 행렬이나 텐서에 대해서도 미분을 정의하고 이를 기울기라고 부른다

자 그럼 다음은 연쇄 법칙

아 결국 각 매개변수에 대한 손실의 기울기를 알아야 그 기울기를 조정해서 매개변수를 갱신할 수 있는거네

그래서 신경망의 기울기를 어떻게 구하냐
오차역전파법(back-propagation)..오..

근데 또 이걸 이해하려면 연쇄 법칙을 알아야 한다.

> 연쇄 법칙은 합성함수에 대한 미분 법칙
아ㅏ 여기서 결국 포인트는

- 어차피 신경망에서 여러 함수가 연결되어 있더라도 그 미분값을 하나하나 다 구할 필요 없이 연쇄법칙을 이용해서 효율적으로 구할 수 있다

기울기가 순전파와 반대 방향으로 전파되는 반대 방향 전파,,바로 역전파
역전파,,중요함,,

여러가지 연산 노드들 중에서 중점적으로 봐야할 것 몇 가지

- sum 노드와 repeat 노드는 반대관계(sum 노드의 순전파가 repeat 노드의 역전파, 그 반대도 성립)
- matmul 노드 어렵네
아 실습 코드에서 좀 주의해야 할 부분이 있는데

grads[0][...] = dW
요런 부분이 있는데 이렇게 생략 기호를 사용함으로써 덮어쓰기를 수행한다.
이는 곧 깊은 복사가 되는데 grads[0] = dW 이렇게 그냥 할당하면 얕은 복사(가리키는 메모리의 위치만 같아짐)가 이루어진다.

softmax 계층의 역전파는 자신의 출력과 실제 정답 레이블의 차이 값으로 떨어진다.
신경망의 역전파는 이러한 오차를 앞 계층에 전달해주는 것!

신경망의 학습 순서는
> 미니배치 -> 기울기 계산 -> 매개변수 갱신
위의 과정을 충분할 때까지 반복하는 것인데 이제 매개변수 갱신에 대해서 알아봐야 한다

매개변수는 기울기 계산값(현재의 가중치 매개변수에서 손실이 가장 큰 방향 기울기)과 반대 방향으로 갱신해서 손실을 줄여야 한다.
> -> 요게 바로 경사하강법!(Gradient Descent)

경사하강법 중 하나가 확률적경사하강법(Stochastic Gradient Descent)

이제 실제로 문제를 풀어보자

직선만으로 클래스를 분류할 수 없을 때 비선형 분리를 학습시켜야 한다.
이러한 비선형 학습시키는 게 위에서도 말했듯이 활성화함수! 그 예로는 시그모이드 함수!

++가중치를 작은 무작위 값으로 설정하면 학습이 잘 진행될 가능성이 커진다고 한다

신경망 영역 분리 시각화 -> 결정 경계(신경망이 식별하는 클래스별 영역을 색으로 구분)

신경망에서 또 중요한 것!
계산 속도이다.

신경망 고속화에 도움되는

- 비트 정밀도: 데이터 타입이 작은게 유리
- GPU: 대량의 연산을 병렬로 처리, CuPy
--- 
학습한 코드는 여기에서
- https://github.com/syi07030/NLP_study

위 코드는 이 책의 코드와 아래의 코드를 바탕으로 작성했습니다.
- https://github.com/WegraLee/deep-learning-from-scratch-2

