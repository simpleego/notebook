3.1 추론 기반 기법과 신경망
분포 가설(단어의 의미는 주변 단어에 의해 형성된다) -> 통계 기반 기법, 추론 기반 기법 / 단어의 동시발생 가능성을 얼마나 잘 모델링하는가
통계 기반 기법의 문제점
- 말뭉치 전체의 통계를 이용해 1회의 처리로 단어의 분산 표현 획득
- 말뭉치가 대규모 크기일 경우 행렬의 크기도 대규모가 되고 여기에 O(n^3) 비용을 갖는 SVD를 적용해서 차원 감소를 하는 것도 비효율적
추론 기반 기법은 미니배치 학습을 한다
- 대규모 말뭉치 처리 가능
- 여러 머신 & 여러 GPU 병렬 계산 -> 학습 속도 up
추론 기반 기법 개요
- 추론 문제 반복 풀기 -> 단어의 출현 패턴 학습
- 타깃이 되는 단어를 추론하기 위해 그 주변 단어들인 맥락을 윈도우 크기에 맞춰서 넣은면 모델, 즉 신경망이 지금까지 학습된 것을 바탕으로 어떤 타깃 단어가 나와야 하는지 확률 분포를 도출한다
신경망(모델)에서의 단어 처리
- 단어 -> 원핫 벡터
- 입력층과 가중치를 곱하고 편향은 생략하여 matmul만으로 완전연결계층 계산 구현이 가능하다
- 여기서 중요한 점 ⭐️c를 입력의 원핫 벡터, W를 가중치 행렬이라고 했을 때 matmul(c,W)는 결국 가중치에서의 해당 위치 행벡터⭐️
- 뒤에 나오는 CBOW 모델에서 이 가중치 W는 ⭐️해당 단어의 분산 표현⭐️
-> 학습을 진행할수록 이 가중치 W값이 더 적절한 값들로 갱신될 것이고 정확하게 단어들을 잘 추론하는 W값을 찾는 것이 목표!
3.2 단순한 word2vec
word2vec에서 사용되는 신경망에는 CBOW(continuous bag-of-words) 모델, skip-gram 모델
CBOW 모델 개요
- 맥락으로부터 타깃을 추론
- 은닉층의 뉴런값은 입력층들의 완전연결계층으로 변환된 값의 평균
- 입력: 맥락, 출력: 각 단어의 점수(점수가 높을수록 확률이 높다)
- 입력층의 뉴런수 > 은닉층의 뉴런수⭐️⭐️ -> 결과적으로 밀집벡터를 얻을 수 있다
- 은닉층의 정보는 인코딩, 디코딩 과정 필요
- 활성화 함수를 사용 X
CBOW 모델 학습
- 가중치 조절 필요
- softmax 함수를 이용해 확률값으로 바꾸고 이를 정답 레이블과 함께 cross entorpy error를 거쳐 손실을 구해낸다
CBOW 모델 가중치
- W_in(입력 측 완전연결계층) 가중치 & W_out(출력 측 완전연결계층) 가중치
- W_in 가중치: 각 행이 각 단어의 분산 표현(수평 방향)
- W_out 가중치: 단어의 의미가 인코딩된 벡터값 저장(수직 방향)
- 최종적으로 사용하는 가중치?
입력 층의 가중치만 사용: word2vec, 특히 skip-gram 모델에서 가장 대중적
출력 층의 가중치만 사용
양쪽 가중치 모두 이용: GloVe(통계 기반 + 추론 기반) 기법에서는 이 때 가장 좋은 결과
3.3 학습 데이터 준비
말뭉치 -> 맥락, 타깃(최종적으로 원핫 표현으로 변환)
3.4 CBOW 모델 구현


이 때 입력측 matmul 계층들은 모두 같은 가중치 공유
X의 역전파는 순전파 시의 입력을 서로 바꿔 기울기에 곱함
미니배치 선택 -> 신경망 입력 -> 기울기 구함 -> 매개변수 갱신 -> 반복
3.5 word2vec 보충
CBOW 모델과 확률
- P(타깃 단어 | 맥락 단어들): 맥락 단어들이 주어졌을 경우 타깃 단어가 나타날 확률
- 말뭉치 전체에 대한 손실 값은 다음과 같이 구할 수 있다(음의 로그 가능도)
(윈도우 크기가 1인 경우)
skip-gram 모델

- 타깃 단어로부터 맥락 단어들 추론
- 최종 손실값 = 맥락의 수만큼 있는 각 출력층의 개별 손실값들의 합
-> 단어 분산 표현의 정밀도 측면에서 skip-gram > cbow 경우가 많음
-> 학습 속도 측면에서 cbow > skip-gram
통계 기반 vs 추론 기반
- 추가할 새로운 단어가 생겨서 단어의 분산 표현을 갱신해야 하는 상황에서..
👍추론 기반 기법이 우세: 여태까지 학습한 가중치를 초기값으로 다시 학습 -> 기존에 학습한 경험 유지 + 효율적 갱신
🤔반면 통계 기반 기법은 동시발생 행렬 다시 만들고 SVD
- 단어 분산 표현의 성격이나 정밀도는..
🌱통계 기반 기법에서는 단어의 유사성이 인코딩
🌱단어의 유사성 + 복잡한 단어 사이의 패턴 인코딩
- 하지만 추론 기반 기법과 통계 기반 기법 중 어떤 것이 더 좋은지는 알 수 없음
- 두 기법은 서로 관련
- GloVe: 추론 기반 기법 + 통계 기반 기법, 말뭉치 전체의 통계 정보를 손실 함수에 도입해 미니배치 학습
학습한 코드는 여기에서
https://github.com/syi07030/NLP_study
위 코드는 이 책의 코드와 아래의 코드를 바탕으로 작성했습니다.
https://github.com/WegraLee/deep-learning-from-scratch-2
사진 출처 또한 여기에서: https://github.com/WegraLee/deep-learning-from-scratch-2
