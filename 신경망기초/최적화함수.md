# 최적화 함수를 정리한다.
## 주요 최적화 함수

## 1). 확률적 경사 하강법 (Stochastic Gradient Descent, SGD):
- 가장 기본적인 최적화 알고리즘으로, 각 학습 데이터 샘플 또는 미니 배치(작은 단위의 학습 데이터 모음)에 대해 손실 함수의 그래디언트(기울기)를 계산하고, 그래디언트의 반대 방향으로 파라미터를 업데이트합니다.

## 2) Adam (Adaptive Moment Estimation):

Adam은 기울기(gradient) 기반의 최적화 방법으로, 특히 미니배치(mini-batch) 학습에 적합합니다. 이 알고리즘은 다음 두 가지 주요 개념을 결합한 것입니다:

모멘텀(Momentum): 모멘텀은 과거의 기울기를 일정 비율로 현재 기울기에 더해줌으로써, 학습의 방향성을 유지하고, 최적점을 향해 더 빠르게 도달할 수 있도록 도와줍니다. 이는 공이 경사면을 굴러 내려가는 것과 유사한 개념입니다.

적응형 학습률(Adaptive Learning Rates): Adam은 각 매개변수에 대해 학습률을 개별적으로 조정합니다. 이를 통해 희소(sparse) 데이터나 불균형 데이터에서도 효과적으로 학습할 수 있습니다.

Adam 최적화 알고리즘은 실제 딥러닝 모델 학습에서 널리 사용되며, 많은 상황에서 뛰어난 성능을 보여줍니다.

[주요 최적화 함수를 등산의 하강에 비유한 설명 예시]

최적화 함수를 등산의 하강에 비유하면, 목적은 산 골짜기의 바닥(예측치와 실제치가 최소에 이르는 바닥))에 도달하는 것이며,
최적화 함수는 등산객에게 바닥에 도달하는 가장 빠르고 효율적인 하강 경로를 제시하는 역할을 합니다.

![image](https://github.com/user-attachments/assets/de3c8ba3-e7b4-4108-86d1-2aa89769654b)



## 1) 확률적 경사 하강법 (SGD):

이 방식은 등산객이 현재 위치에서 가장 가파른 경사를 찾아, 그 방향으로 한 걸음 내딛는 것에 비유할 수 있습니다. 그러나 이 방법은 때때로 등산객을 작은 골짜기에 갇히게 하거나, 진동하게 만들 수 있습니다.

## 2) Adam:

Adam은 등산객이 이전의 이동 경험을 기억하여 더 부드러운 하강 경로를 찾는 것에 비유할 수 있습니다. 이 방식은 등산객이 더욱 빠르고 안정적으로 목적지에 도달할 수 있도록 돕습니다.

## 3) RMSprop:

RMSprop은 등산객이 각 발걸음마다 학습률을 조정하여 더 안정적으로 하강할 수 있도록 돕는 것에 비유할 수 있습니다. 이 방식은 더 부드러운 하강 경로를 제공하며, 등산객이 빠르게 목적지에 도달할 수 있도록 합니다.

## 4) Adagrad:

Adagrad는 등산객이 처음에는 큰 발걸음을 내딛지만, 점점 더 작은 발걸음을 내딛도록 조정하는 것에 비유할 수 있습니다. 이 방식은 등산객이 처음에는 빠르게 하강하지만, 후반에는 느리게 하강하게 됩니다.
